{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n# Handwritten Digit Recognition (MNIST Dataset)\n<hr>\n\n![MNIST](https://neurohive.io/wp-content/uploads/2019/05/Screenshot-from-2019-05-29-21-23-47.png)\n### History of Handwritten Digit dataset \n\nModified National Institute of Standards and Technology database (MNIST dataset) is a large dataset of handwritten digits which is widely used in image processing and machine learning. The set of images in the MNIST database is a combination of two of NIST's databases: Special Database 1 and Special Database 3. Special Database 1 and Special Database 3 consist of digits written by high school students and employees of the United States Census Bureau, respectively.<br> \n\n\n[READ MORE](https://en.wikipedia.org/wiki/MNIST_database)\n\n### Why to start with MNIST dataset?\nThe MNIST dataset is a well-known dataset for image classification. Tensorflow and Keras also provide MNIST dataset directly through their APIs. For the learning purpose, the MNIST dataset is easy to use and experiment with different machine learning techniques.\n\n### About the Notebook\n* In this notebook, I have covered the necessary steps to approach any Machine Learning Classification Problem.\n* Included Image Visualization for better understanding.\n* Quick Links to the functions I have used to explore it in depth.\n* Basic techniques such as Confusion Matrix, Image Augmentation, etc.\n* I have also compared the results of Model without using CNN and CNN.\n\nI have tried to make this notebook as simple as possible, along with covering the basic approach to tackle any classification task. \n### Task\nThe task is to classify the images in 10 class, i.e., [0-9], inclusively.  \n<hr>\n\n<font style=\"color:Red;font-size:18px\">If you find this notebook helpful, Please UPVOTE.</font>\n\n### Follow me:\n\n* <a href=\"https://bit.ly/tarungithub\"><img src=\"data:image/svg+xml;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCAyNCAyNCIgaGVpZ2h0PSI1MTIiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjUxMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMTIgLjVjLTYuNjMgMC0xMiA1LjI4LTEyIDExLjc5MiAwIDUuMjExIDMuNDM4IDkuNjMgOC4yMDUgMTEuMTg4LjYuMTExLjgyLS4yNTQuODItLjU2NyAwLS4yOC0uMDEtMS4wMjItLjAxNS0yLjAwNS0zLjMzOC43MTEtNC4wNDItMS41ODItNC4wNDItMS41ODItLjU0Ni0xLjM2MS0xLjMzNS0xLjcyNS0xLjMzNS0xLjcyNS0xLjA4Ny0uNzMxLjA4NC0uNzE2LjA4NC0uNzE2IDEuMjA1LjA4MiAxLjgzOCAxLjIxNSAxLjgzOCAxLjIxNSAxLjA3IDEuODAzIDIuODA5IDEuMjgyIDMuNDk1Ljk4MS4xMDgtLjc2My40MTctMS4yODIuNzYtMS41NzctMi42NjUtLjI5NS01LjQ2Ni0xLjMwOS01LjQ2Ni01LjgyNyAwLTEuMjg3LjQ2NS0yLjMzOSAxLjIzNS0zLjE2NC0uMTM1LS4yOTgtLjU0LTEuNDk3LjEwNS0zLjEyMSAwIDAgMS4wMDUtLjMxNiAzLjMgMS4yMDkuOTYtLjI2MiAxLjk4LS4zOTIgMy0uMzk4IDEuMDIuMDA2IDIuMDQuMTM2IDMgLjM5OCAyLjI4LTEuNTI1IDMuMjg1LTEuMjA5IDMuMjg1LTEuMjA5LjY0NSAxLjYyNC4yNCAyLjgyMy4xMiAzLjEyMS43NjUuODI1IDEuMjMgMS44NzcgMS4yMyAzLjE2NCAwIDQuNTMtMi44MDUgNS41MjctNS40NzUgNS44MTcuNDIuMzU0LjgxIDEuMDc3LjgxIDIuMTgyIDAgMS41NzgtLjAxNSAyLjg0Ni0uMDE1IDMuMjI5IDAgLjMwOS4yMS42NzguODI1LjU2IDQuODAxLTEuNTQ4IDguMjM2LTUuOTcgOC4yMzYtMTEuMTczIDAtNi41MTItNS4zNzMtMTEuNzkyLTEyLTExLjc5MnoiIGZpbGw9IiMyMTIxMjEiLz48L3N2Zz4=\" width=\"22\" align=\"left\" style=\"margin-right:10px\"/> GitHub</a>\n* <a href=\"https://bit.ly/tarnkr-youtube\"><img src=\"data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iaXNvLTg4NTktMSI/Pg0KPCEtLSBHZW5lcmF0b3I6IEFkb2JlIElsbHVzdHJhdG9yIDE5LjAuMCwgU1ZHIEV4cG9ydCBQbHVnLUluIC4gU1ZHIFZlcnNpb246IDYuMDAgQnVpbGQgMCkgIC0tPg0KPHN2ZyB2ZXJzaW9uPSIxLjEiIGlkPSJMYXllcl8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCINCgkgdmlld0JveD0iMCAwIDQ2MS4wMDEgNDYxLjAwMSIgc3R5bGU9ImVuYWJsZS1iYWNrZ3JvdW5kOm5ldyAwIDAgNDYxLjAwMSA0NjEuMDAxOyIgeG1sOnNwYWNlPSJwcmVzZXJ2ZSI+DQo8cGF0aCBzdHlsZT0iZmlsbDojRjYxQzBEOyIgZD0iTTM2NS4yNTcsNjcuMzkzSDk1Ljc0NEM0Mi44NjYsNjcuMzkzLDAsMTEwLjI1OSwwLDE2My4xMzd2MTM0LjcyOA0KCWMwLDUyLjg3OCw0Mi44NjYsOTUuNzQ0LDk1Ljc0NCw5NS43NDRoMjY5LjUxM2M1Mi44NzgsMCw5NS43NDQtNDIuODY2LDk1Ljc0NC05NS43NDRWMTYzLjEzNw0KCUM0NjEuMDAxLDExMC4yNTksNDE4LjEzNSw2Ny4zOTMsMzY1LjI1Nyw2Ny4zOTN6IE0zMDAuNTA2LDIzNy4wNTZsLTEyNi4wNiw2MC4xMjNjLTMuMzU5LDEuNjAyLTcuMjM5LTAuODQ3LTcuMjM5LTQuNTY4VjE2OC42MDcNCgljMC0zLjc3NCwzLjk4Mi02LjIyLDcuMzQ4LTQuNTE0bDEyNi4wNiw2My44ODFDMzA0LjM2MywyMjkuODczLDMwNC4yOTgsMjM1LjI0OCwzMDAuNTA2LDIzNy4wNTZ6Ii8+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8L3N2Zz4NCg==\" width=\"22\" align=\"left\" style=\"margin-right:10px\"/>Youtube<a/>\n* <a href=\"https://medium.com/@codeeasy\"><img src=\"data:image/svg+xml;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCAyNCAyNCIgaGVpZ2h0PSI1MTIiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjUxMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMjIuMDg1IDQuNzMzIDEuOTE1LTEuODMydi0uNDAxaC02LjYzNGwtNC43MjggMTEuNzY4LTUuMzc5LTExLjc2OGgtNi45NTZ2LjQwMWwyLjIzNyAyLjY5M2MuMjE4LjE5OS4zMzIuNDkuMzAzLjc4M3YxMC41ODNjLjA2OS4zODEtLjA1NS43NzMtLjMyMyAxLjA1bC0yLjUyIDMuMDU0di4zOTZoNy4xNDV2LS40MDFsLTIuNTItMy4wNDljLS4yNzMtLjI3OC0uNDAyLS42NjMtLjM0Ny0xLjA1di05LjE1NGw2LjI3MiAxMy42NTloLjcyOWw1LjM5My0xMy42NTl2MTAuODgxYzAgLjI4NyAwIC4zNDYtLjE4OC41MzRsLTEuOTQgMS44Nzd2LjQwMmg5LjQxMnYtLjQwMWwtMS44Ny0xLjgzMWMtLjE2NC0uMTI0LS4yNDktLjMzMi0uMjE0LS41MzR2LTEzLjQ2N2MtLjAzNS0uMjAzLjA0OS0uNDExLjIxMy0uNTM0eiIgZmlsbD0iIzIxMjEyMSIvPjwvc3ZnPg==\"  width=\"22\" align=\"left\" style=\"margin-right:10px\"/> Medium</a>\n* <a href=\"https://www.linkedin.com/in/tarun-kumar-iit-ism/\"><img src=\"data:image/svg+xml;base64,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\" width=\"22\" align=\"left\" style=\"margin-right:10px\"/>Linkedin</a>\n\n\n\n<hr>\n\n# Content:\n\n* [Handwritten Digit Recognition (MNIST Dataset)](#Handwritten-Digit-Recognition-(MNIST-Dataset))\n* [Required Imports](#Required-Imports)\n* [Loading and Visualizing Dataset](#Loading-and-Visualizing-Dataset)\n* [Visualize Digits dataset](#Visualize-Digits-dataset)\n* **[Buliding Model](#Buliding-Model)**\n    * [Model Using Keras](#Model-Using-Keras)\n    * [Compiling Model](#Compiling-Model)\n    * [Training](#Training)\n    * [Training Performance](#Training-Performance.)\n    * [Confusion Matrix](#Confusion-Matrix)\n* **[Improving Result by Image Augmentation](#Improving-Result-by-Image-Augmentation)**\n    * [Augmentation Using Keras](#Augmentation-Using-Keras)\n    * [Learning Rate](#Learning-Rate)\n* [Visualizing Result](#Visualizing-Result)\n* [Predict on Testset](#Predict-on-Testset)\n* **[Understand the Intermediate Layers of the Model](#Lets-understand-the-intermediate-layers-of-the-model)**\n    * [Compare the layer 1 output](#Try-to-compare-the-layer-1-output)\n    * [Visualizing All Intermediate Activation Layer](#Visualizing-All-Intermediate-Activation-Layer)"},{"metadata":{},"cell_type":"markdown","source":"# Required Imports"},{"metadata":{},"cell_type":"markdown","source":"**Imports:**\n* pandas : For handeling csv dataset\n* numpy : Support for Pandas and calculations\n* Matplotlib - For visualization (Plotting graphs)\n*  keras - Prediction Models"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt \nimport cv2 as cv\n\nfrom keras.layers import Conv2D, Input, LeakyReLU, Dense, Activation, Flatten, Dropout, MaxPool2D\nfrom keras import models\nfrom keras.optimizers import Adam,RMSprop \nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nimport pickle\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading and Visualizing Dataset\n<hr>\n\n## About Dataset\nMNIST dataset has the following features:\n* Dataset size 60,000 samples of handwritten images.\n* The size of each image is 28x28 pixels.\n* Each image has only 1 color channel, i.e., grayscale image.\n* Each pixel has value in the range of [0,255] where 0 represents black, and 255 represents white.\n* Each image has labeled from 0-9.\n\n\n## Loading train.csv\n\nThis will loads the data from Kaggle dataset train.csv into a **Dataframe**. As file type is CSV, I am loading it using [read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) of **pandas**. Then using NumPy for **random permutation** of the training dataset. I am using seed to regenerate the same permutation every time. Change seed value to get different permutation. \n<br>\n\n**Check out these functions for more info:**\n* [df.iloc[]](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html)\n* [np.random](https://docs.scipy.org/doc/numpy-1.14.0/reference/routines.random.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(1) # seed\ndf_train = pd.read_csv(\"../input/digit-recognizer/train.csv\") \n# Loading Dataset\ndf_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.iloc[np.random.permutation(len(df_train))] \n# Random permutaion for dataset (seed is used to resample the same permutation every time)\n# 数据集的随机排列,np.random.permutation生成随机序列\ndf_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.permutation(len(df_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training set has 42,000 images. And has 785 columns, 1st coloumn is label for the image and rest 784 are the pixel values. <br>**Remember it is flattened. I will reshape it latter.**"},{"metadata":{},"cell_type":"markdown","source":"### Preparing Training and Validation data \n**It requires a few steps:**\n* Assuming the validation set size. I am taking it 10% of the training set.  \n* Splitting training set into a training set (90% original training set) and validation set (10% original training set) from the training dataset.\n* Reshaping both sets into (sample size,28,28,1) where sample size represents the size of the train or validation set.\n* Splitting the labels for both training set and validation set.\n\nUsing df.iloc of pandas for slicing the data frame(read more link below), then converting into NumPy array and finally reshaping NumPy array in required shape.<br>\n\n**Check out these functions for more info:**\n* [df.iloc[]](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html)\n* [np.reshape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html)\n* [np.asarray](https://docs.scipy.org/doc/numpy/reference/generated/numpy.asarray.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_size = df_train.shape[0] # Training set size\nvalidation_size = int(df_train.shape[0]*0.1) # Validation set size \n\n# train_x and train_y\ntrain_x = np.asarray(df_train.iloc[:sample_size-validation_size,1:]).reshape([sample_size-validation_size,28,28,1]) \n# taking all columns expect column 0\ntrain_y = np.asarray(df_train.iloc[:sample_size-validation_size,0]).reshape([sample_size-validation_size,1]) \n# taking column 0\n\n# val_x and val_y\nval_x = np.asarray(df_train.iloc[sample_size-validation_size:,1:]).reshape([validation_size,28,28,1])\nval_y = np.asarray(df_train.iloc[sample_size-validation_size:,0]).reshape([validation_size,1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Shape of training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x.shape,train_y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading test.csv\nThis will load the test.csv. It has 18,000 images and no label is there. Prediction need to be done on these images. After loading converting the data in form of numpy array and reshaping it."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(\"../input/digit-recognizer/test.csv\")\ntest_x = np.asarray(df_test.iloc[:,:]).reshape([-1,28,28,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_x.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.max(test_x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Normalize Pixel Data\nEach pixel values lies between [0,255]. This value range is too high and it will be difficult for any model to learn. The best approach is **normalize** the data. In this case, as the pixel value is in the known range it sufficient to **scale the pixel values** in range [0,1] by simply dividing the array by **255**. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# convirting pixel values in range [0,1]\ntrain_x = train_x/255\nval_x = val_x/255\ntest_x = test_x/255","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualize Digits dataset\n<hr>\n\nThe first and fundamental thing to check is the frequency of the classes in the dataset, as the balanced dataset is always good to start. But this is not always true, and there are several supervised learning tasks in which the classes are not balanced, also in case of anomaly detection, there is a large difference between the positive and negative class. But that is out of the scope of this notebook.\n\n### 1. Frequency plot for the training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.iloc[:sample_size-validation_size,:].groupby('label')['label'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cheacking frequency of digits in training and validation set\ncounts = df_train.iloc[:sample_size-validation_size,:].groupby('label')['label'].count()\n# df_train.head(2)\n# counts\nf = plt.figure(figsize=(10,6))\nf.add_subplot(111)\n\nplt.bar(counts.index,counts.values,width = 0.8,color=\"orange\")\nfor i in counts.index:\n    plt.text(i,counts.values[i]+50,str(counts.values[i]),horizontalalignment='center',fontsize=14)\n\nplt.tick_params(labelsize = 14)\nplt.xticks(counts.index)\nplt.xlabel(\"Digits\",fontsize=16)\nplt.ylabel(\"Frequency\",fontsize=16)\nplt.title(\"Frequency Graph training set\",fontsize=20)\nplt.savefig('digit_frequency_train.png')  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counts[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Frequency plot for the validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train.iloc[sample_size-validation_index:,1:]\n# Cheacking frequency of digits in training and validation set\ncounts = df_train.iloc[sample_size-validation_size:,:].groupby('label')['label'].count()\n# df_train.head(2)\n# counts\nf = plt.figure(figsize=(10,6))\nf.add_subplot(111)\n\nplt.bar(counts.index,counts.values,width = 0.8,color=\"orange\")\nfor i in counts.index:\n    plt.text(i,counts.values[i]+5,str(counts.values[i]),horizontalalignment='center',fontsize=14)\n\nplt.tick_params(labelsize = 14)\nplt.xticks(counts.index)\nplt.xlabel(\"Digits\",fontsize=16)\nplt.ylabel(\"Frequency\",fontsize=16)\nplt.title(\"Frequency Graph Validation set\",fontsize=20)\nplt.savefig('digit_frequency_val.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that both the training and validation set has a good balance between the classes, so let's move on and see a few of the digits.\n\n### Visualizing the digits by plotting Images\n<hr>\nThis will plot the first 30 images of digits with the label."},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = 5 # defining no. of rows in figure\ncols = 6 # defining no. of colums in figure\n\nf = plt.figure(figsize=(2*cols,2*rows)) # defining a figure \n# 在matplotlib一般使用plt.figure来设置窗口尺寸。\nfor i in range(rows*cols): \n    f.add_subplot(rows,cols,i+1) \n    # adding sub plot to figure on each iteration\n    plt.imshow(train_x[i].reshape([28,28]),cmap=\"Blues\") \n    plt.axis(\"off\")\n    plt.title(str(train_y[i]), y=-0.15,color=\"green\")\nplt.savefig(\"digits.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(train_x[1].reshape([28,28]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(train_x[1].reshape([28,28]),cmap=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = 5 # defining no. of rows in figure\ncols = 1 # defining no. of colums in figure\n\nf = plt.figure(figsize=(2^cols,2*rows)) # defining a figure \n# f = plt.figure(figsize=(2,2))\nfor i in range(rows*cols): \n    f.add_subplot(rows,cols,i+1) \n    # adding sub plot to figure on each iteration\n    plt.imshow(train_x[i].reshape([28,28]),cmap=\"Blues\") \n    plt.axis(\"off\")\n    plt.title(str(train_y[i]), y=-0.15,color=\"green\")\nplt.savefig(\"digits.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Buliding Model\n\n<hr>\n\n![model](https://miro.medium.com/max/1000/1*4OUonEDfZwCfR4Y-G-h1fw.jpeg)"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# # Loading pickled resources\n# !wget https://www.kaggleusercontent.com/kf/31703703/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..f6l0FhjgIxCd7bfZA3MF_A.oHtR56AzfPZhslGD-R2Uca6Kce1yVCG805lWUk25H5T-MQciLjAjmoTd9RPGh1zdUlwtbMZPuAi9_1BrHNZfFlJ5duoYajHON-Sk_mMy7OIePjqNRqo8vkEnTHmbV6Oj1Z6MR9dGzT2Gch2soeaLaZnIjxJt5e8DsFaia6dTjxRzzzrKaQDWLikdsjo2xwbQp9yo4-8htw6adclSbtnXsMV4kJNBs25d-qqRLuUSuhqxxCbJMkuuZgPjnEzqO7aLU0zqcGYUXDDdx1O-oU2ncMAYpXYqssqzQgD6-t4Fl83XWQnNqRv5wec5cdD-7IF9cbjyD_CE-Ib863pPJ9RJc-IYypbUvvKfMQuhahe9NiuRGNSNodVlSiuSzk0nudl5uHqf7V7_1h_juPPVj8mUUOqleLye9_ZtJ2S8pD6hUXT9p7kPy6v6RdoaE_LgkrijyvmJhmS-yMETpazrlQlKp96A3W0EVdhtVxmW7QUwbjIlzdEs7whAe4EcqQIzd4H69TR6hLCqVlaZkMBBPvBWr_dCTxu6htDP8qE2QCH08H1VPXyZLERTMH1SRENnwa_BxMTVkc_pP70tkvGA2xtgoJHzAlcOZZfqsa5fmCa8tqIOkME1hn88Xgm5eh58JXT2ZXyA7hxfpzKP3UQXegeBnPaEPOLa5eEoND9E9ypyi5I1QWa7QTMe9ruEbXr6DUbJ.GvQemyhjKibaOnfNrnRUXQ/model.h5\n# !wget https://www.kaggleusercontent.com/kf/31703703/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..yUikFJ_W78H0Zfo7LsQ-2g.1-XOTVZwAUnN9uzbSyaPEuXbdS_5mlLvYRUOaFjMZewxYpcCFASz2fROWEIHjC-KHCOOO-DYjLY7pWHvN81cOW25n28C2mT6aFnWkObrkUYXlbZW0sS7iaKwxUw4a_XwYYUmMfeNuOpKf5OsBdv-L2y0GOLb-fT8hDqteihh6qP97VOT1fmvdv2gYcG0WqKw4mcWwtWAYyqQAScE_knALnyTvWZTF73LR99gaxy_w41t3PHo0rfHy037Cll0lWznkf6ppfAIv_CKr_v2HfpB9go0DJbXKqD87LkcnNF94e_b95i0UYfS2pMugXS4ob1WnSblE34Df_n9rB3pWMXn9DPnqlwDqa1snJOc3CLeK9MTsPQ9NuzEFIjxMnWXQhNGQ7sXYX8qAPqanQ3-OQSZHHD8lufpcJEguVqOuOWY5qZlKe4ZTzn3g8fhbssulphnajM4ZThC5pceWnVT2rowvERPtlvijy6AdAGEy57BxN7FeweBVZAR1Zv5RA_wE2qo4DFmyK36j8p_bVZHTnVvqFFWz0SAimJqzMmzxNrZY7_NGuqDy5rjUmoa2y0e0-qPFBcfFWvT2Pe_RKW21dBOKH9HbI1j0WS_Ua72FI1-MToX8DUHRdN9UNLJhnugwWY_lWvppU-fRCJFhkSPIYyfzbx1cH2ykLLZRz4-sjCegE78I03ue3096Q7kUMw9-ZGd.a3Ge-GJWsUMNPuyQ0gpMfg/model_img_augmentation.h5\n# !wget https://www.kaggleusercontent.com/kf/31703703/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..Hq9sgF7KDfRSLRlO3zlQSg.dQKW1YCUjhwAZbX-54g1ygj0qTWSH6egywwOdIqm-hwUxTWnj6L9EkNK8qncdH4FeNbEtAjYrQ2oStYGIVVJzsqFDQwaXBsnzvKQx2icckW3ww-aNtDxATdkVY2ZiqMLAMIMyy8_nFoFvt2tQm48XeIUecn3OTL6Te3VfRr3OXQ1QWU7lbY-8BetDVy0tbrLV_vIkV-fUy_FLGsa7QgH4Xerxi7xXoItAmJCbEIXBt5pR-_frNz0rdDoj30e-xGdUkLRiBNe1Nk9_1EFacGwTwhM3KqE8SDF5CtVqP7XsFhFIxVal-lmUy1hzsPi0xZt_ikRIbmfTb6K5HmQ97Jzm4nd7YdQ_u2ScbhLLFy2pj0n4XapGZgMYE5o_2_Cv2c_3uquTGDTpjiOAA25ylrFypwZGmenAeCSrJZIto0ta_onqVPz_euNQY4PHJ5P3aJi2KWHJzNl3LxBU2u-LfkfOzwYXu1DIOOqkzSSLWraxZoleean-9oawdqq0doQhogfx2wBr844ApCpVYDxEEn8CxAwfK-RoiE7MdEasDYtWujlmcOnMQsQjUohnJcOWwQSIOIEpqevcXBU4PPoiw-ex-vEUtflupnNRtlwwLRzjyrV_6VJbdQkus1F0HfToHEBjB6tkZyydreIK3Zi8u0l1YvT3VRtWMw3amc3s7f86ilZZ_uoGhW8fc7k14JmubAX.FiOfKdbaYf0TStGFpmhDSw/history_1.hs\n# !wget https://www.kaggleusercontent.com/kf/31703703/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..IWrH9XKSk0oDOCNg7wTlbg.A49rZubI479_e2da4l7JhZBGm7YdJ87UviDnyAP6924Kez0dRIPjzw-67wCI_iMtJGQQJrnn74pVwzt0cPa8CVTJIApHs2t4qZd-7dlWDcOqcwlC8zaLECXT836Jey6auUY1JE1C8YApSHBfDr-WB_KO75JQRzDdXV6LFEdPfzTyoXWf0zmuyg4bi41rPhH2UkhEfPowmze9G_nOWg64WHfJTBCcWzMDf56GbyNmp46RtdKuqriMH0sAHHFrz94DQkHnnz0U149yYC6oUkqlvty-l7jvEGM8Nc1_-LZbptB_8cZxu9gOFYprjdUyAerb5Izz-J1nkextV-0OUe8SUYj8XwG5pF6pNDbZrkoDTurkkORlMIEAKNhl6SZvPjmb1Mv-owxnrpSdHnHejp61kFp-FOi_oFFzAJiW5NVm8pmMln7RP3JgcuCfWKQUEHlrsC5NaMvsFG9CNlXeul9E4PXS6ycuRzflz5uWxvoe1Dt-Tm4Pnoaa-CvJ2hsLKsX-oH66LE6sWQm8UrDTJ0eBxaWG8QAbJ6WeOICv9zykRx8ro5j18RWgnOLK2x9PLJmYA7rL6Zm7kxavLQMTovSg-IZ9EyuTuSmGv-0Rd_0Gmb5-2qvBsYKGcn8lbJMmOXfeDWEqTT0XBK-VfVcDNxX8EZd2iYD1c3nJa6R324v4JGTOLHZJbL7UNtGXtQn-3WVc.ShxYOf_zson_2CAlaDOT8w/history_2.hs\n!wget https://www.kaggleusercontent.com/kf/32045042/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..LcCpstRicdTOqMGGjh1wmg.LJi-FKBMbLQL5cc2XHtVyNpT--iVgBeKlbby9C_YjBjm7dSLbbqY4F27EpcgOWZYo22EOWWwyVMAd7VsYOzQkUGK-sj6gvWswv9CTc8I8ZDMkZvGdRRy3COevzGk21yXTlbRG1D--BVHlwCZVaYSMBKt9gIeCELgl-ZcFKoOjZSUpcZXBrmfuBA_OK63fc_olldErb5p5S_qWVecdJ1r49anGVs-x682Q0y5cCs9lr7q1n2B2saWkiC4d8tMGRMyUTCt1xXl6nENaYNh4C4u0DGOBfT14jEuWfyzFH3obgs_TtD4Bqg2zzxjjUwtJd7ymBNCVq7td-_chLlds-lps5uq4BLckniKl9QgbJ7ZcL6qoMonLzPn5VUE-vlIfUCfBieYOHKkWb3Buz557IrG4mk9GqAvOPp_Vzd5n9h9SGEvOFCsiG-_IinSIA7a2NxSPvtNEKmYcG37CS0xLfEV8copUh3pIHdVVLn9SXd2r5Wg2Q5q8zUnEvnHSxcPNg7LheviS4NMX18bsL9Wqm2PP_WDd7QkO_wJ0AsGyDveLWKebHB26wS_iz3w7X-EAheRmBrCyiUfJWHSM-IJPAJk9EIhRH1zr8WVtwsc19PRevFcY4RDHx7fyvPFVrJ2fFsicZMaDfJchFR_lP_uYDs6NvPu5AXXEkXnFMsrJ0IAICiEDwadnj0eV5w5b-DozhrP.8pmfOFY56dYt_Z2UVg-UWA/model_img_augmentation.h5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Using Keras\n\nThere are two different ways of defining the Model in Keras:\n* Sequential Model\n* Function API\n\nFunctional API is used to build a more complicated Model such as for multi-output Models, directed acyclic graphs, or models with shared layers. I am using the Sequential Model in this notebook to keep things simple. \n<br>\nIn Sequential Model, you can add each layer sequentially.<br>\n\n**Description of Model:**\n* 2 Convolutional Blocks\n    > Each block consists of 2 Conv2D layers with LeakyRelU activation layers. Then a MaxPool2D layer and finally a Dropout Layer. \n* Then Dense Layers and Output layer after Flatten layer.\n* MaxPool2D layer is used to reduce the size of the image. Pool size `(2,2)` means reducing the image from `(28,28)` to `(14,14)`. Reducing the features.\n* Dropout layer drops the few activation nodes while training, which acts as regularization. Do let the model to over-fit.\n* Output layer has 10 nodes with sigmoid activation.\n\n**Check out these functions for more info:**\n* [Conv2D](https://keras.io/layers/convolutional/#conv2d)\n* [LeakyReLU](https://keras.io/layers/advanced-activations/#leakyrelu)\n* [MaxPool2D](https://keras.io/layers/pooling/#maxpool2d)\n* [Dropout](https://keras.io/layers/core/#dropout)\n* [Flatten](https://keras.io/layers/core/#flatten)\n* [Dense](https://keras.io/layers/core/#dense)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = models.Sequential()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Block 1\nmodel.add(Conv2D(32,3, padding  =\"same\",input_shape=(28,28,1)))\n# filter数目：32\n# filter大小：3*3\nmodel.add(LeakyReLU())\nmodel.add(Conv2D(32,3, padding  =\"same\"))\nmodel.add(LeakyReLU())\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n# output：14*14*32\n\n# Block 2\nmodel.add(Conv2D(64,3, padding  =\"same\"))\nmodel.add(LeakyReLU())\nmodel.add(Conv2D(64,3, padding  =\"same\"))\nmodel.add(LeakyReLU())\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n# output：7*7*64\n\nmodel.add(Flatten())\n# output：7*7*64=3136\n\nmodel.add(Dense(256,activation='relu'))\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Dense(10,activation=\"sigmoid\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Compiling Model\nModel compilation required the selection of optimizer and loss function. Let me discuss a few important things to avoid confusion.\n\n**Optimizers:** Keras provides several optimizers that can be used by importing the optimizers and passing in compile function.\n* SGD (Stochastic gradient descent optimizer)\n* RMSprop \n* Adam\n* Adamax\n\nThere are several more check it out [here](https://keras.io/optimizers/). \n<br>\n\nOne of the important things is the selection of the **learning rate**. If the learning rate is too high, the loss may not converge, and if it is too low, the training will be slow. So it is important to select the reasonably fair value of learning rate. One of the good value to start with is 0.001. If it doesn't work, then other higher or lower values can be tried. The rest of the parameters generally works well and need not be defined. The default value works most of the time.\nHere, I am using **Adam optimizer**, but **RMSprop** can also be used.\n<br>\n\n**Loss Functions:** Keras provides all of the well-known loss functions which work well for most of the time. But if you need to define a custom loss function, you can. Defining a custom function is out of the scope of this notebook. Let's understand the loss function for the classification tasks. I am discussing 3 loss function here. There are several more check it out [here](https://keras.io/losses/). \n* **binary_crossentropy**: This loss function is used for the binary classification task. The single-node output layer is required. 0 and 1 is used for classification.\n* **categorical_crossentropy**:  Used for Used for Multi-class classification \n* **sparse_categorical_crossentropy:** Used for Multi-class classification. <br>\n\n**Difference between categorical_crossentropy and sparse_categorical_crossentropy:**\n* If your targets are one-hot encoded, use categorical_crossentropy.\n    Examples of one-hot encodings (for three classes):\n    * [1,0,0]\n    * [0,1,0]\n    * [0,0,1]\n* But if your targets are integers, use sparse_categorical_crossentropy.\n    Examples of integer encodings (for the sake of completion):\n    * 1\n    * 2\n    * 3\n    \nHere, I am using **sparse_categorical_crossentropy**, as the target values are integer not **one-hot vector**.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"initial_lr = 0.001\nloss = \"sparse_categorical_crossentropy\"\nmodel.compile(Adam(lr=initial_lr), loss=loss ,metrics=['accuracy'])\nmodel.summary()\n# optimizer: 优化器\n# loss:损失函数,可以用自带的,也可以自定义.如果模型有多个输出,\n#     可以传入一个字典或者损失列表,模型降会把这些损失加在一起\n# metrics: 列表，包含评估模型在训练和测试时的性能的指标，典型用法是metrics=[‘accuracy’]。","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training \n[model.fit()](https://keras.io/models/sequential/#fit) is used to train the model. It takes training data, batch_size, no of epochs, validation data. There are several more parameters, and you can check the documentation [here](https://keras.io/models/sequential/#fit).\nI am taking the epochs = 20 and batch size = 256. It will return the history of training, which later can be used to analyze the performance."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"epochs = 20\nbatch_size = 256\nhistory_1 = model.fit(train_x,train_y,batch_size=batch_size,epochs=epochs,validation_data=[val_x,val_y])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_1.epoch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_1.history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"model.save(\"model.h5\")\nwith open('history_1.hs', 'wb') as history:\n    pickle.dump(history_1,history)\n    \n# pickle.dump(obj, file, [,protocol])\n# 函数的功能：将obj对象序列化存入已经打开的file中。\n\n# pickle.load(file)\n# 函数的功能：将file中的对象序列化读出","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# model = models.load_model(\"model.h5\")\n# with open('history_1.hs', 'rb') as history:\n#     history_1 = pickle.load(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training Performance.\nLet's see how the training goes—plotting the accuracy and loss of both training and validation set with each epoch. In the accuracy graphs, there is clearly a difference in the training and validation set. The model is more accurate on the training set. It seems that the model is a little bit **overfit**. So can we do better?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Diffining Figure\nf = plt.figure(figsize=(20,7))\n\n#Adding Subplot 1 (For Accuracy)\nf.add_subplot(121)\n\nplt.plot(history_1.epoch,history_1.history['accuracy'],label = \"accuracy\") \n# Accuracy curve for training set\nplt.plot(history_1.epoch,history_1.history['val_accuracy'],label = \"val_accuracy\") \n# Accuracy curve for validation set\n\nplt.title(\"Accuracy Curve\",fontsize=18)\nplt.xlabel(\"Epochs\",fontsize=15)\nplt.ylabel(\"Accuracy\",fontsize=15)\nplt.grid(alpha=0.3)\nplt.legend()\n\n#Adding Subplot 1 (For Loss)\nf.add_subplot(122)\n\nplt.plot(history_1.epoch,history_1.history['loss'],label=\"loss\") \n# Loss curve for training set\nplt.plot(history_1.epoch,history_1.history['val_loss'],label=\"val_loss\")\n# Loss curve for validation set\n\nplt.title(\"Loss Curve\",fontsize=18)\nplt.xlabel(\"Epochs\",fontsize=15)\nplt.ylabel(\"Loss\",fontsize=15)\nplt.grid(alpha=0.3)\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrix\nIn the field of **machine learning** and specifically the problem of **statistical classification**, a confusion matrix, also known as an **error matrix**, is a specific table layout that **allows visualization of the performance of an algorithm**, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix). Each row of the matrix represents the instances in a predicted class, while each column represents the instances in an actual class (or vice versa). The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabeling one as another). [Source](https://en.wikipedia.org/wiki/Confusion_matrix)\n\nLet's try to see how well the model is performing on the validation set."},{"metadata":{"trusted":true},"cell_type":"code","source":"val_p = np.argmax(model.predict(val_x),axis =1)\n\nerror = 0\nconfusion_matrix = np.zeros([10,10])\nfor i in range(val_x.shape[0]):\n    confusion_matrix[val_y[i],val_p[i]] += 1\n    if val_y[i]!=val_p[i]:\n        error +=1\n        \nprint(\"Confusion Matrix: \\n\\n\" ,confusion_matrix)\nprint(\"\\nErrors in validation set: \" ,error)\nprint(\"\\nError Persentage : \" ,(error*100)/val_p.shape[0])\nprint(\"\\nAccuracy : \" ,100-(error*100)/val_p.shape[0])\nprint(\"\\nValidation set Shape :\",val_p.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_p.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = plt.figure(figsize=(10,8.5))\nf.add_subplot(111)\n\nplt.imshow(np.log2(confusion_matrix+1),cmap=\"Reds\")\nplt.colorbar()\nplt.tick_params(size=5,color=\"white\")\nplt.xticks(np.arange(0,10),np.arange(0,10))\nplt.yticks(np.arange(0,10),np.arange(0,10))\n\nthreshold = confusion_matrix.max()/2 \n\nfor i in range(10):\n    for j in range(10):\n        plt.text(j,i,int(confusion_matrix[i,j]),horizontalalignment=\"center\",color=\"white\" if confusion_matrix[i, j] > threshold else \"black\")\n        \nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.savefig(\"Confusion_matrix1.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Improving Result by Image Augmentation\n\n<hr>\n\n![augmentation](https://cdn-images-1.medium.com/max/1000/1*C8hNiOqur4OJyEZmC7OnzQ.png)\n\nA deep network requires extensive data to achieve decent performance. To build a good classifier with small training data, image augmentation can solve the problem to a greater extend. Image augmentation generates images by different ways of processing, such as random shift, rotation, flips, etc. \n<br>\n\nYou can check great python package for data [augmentation](https://github.com/albumentations-team/albumentations).\n\n![Augmentation](https://camo.githubusercontent.com/43d652646b37ef66762212c0e0d3150ba481347c/68747470733a2f2f686162726173746f726167652e6f72672f776562742f73752f77612f6e702f737577616e70656f36777737777077746f6274727a645f636732302e6a706567)*Image [source](https://github.com/albumentations-team/albumentations)* \n\n\n### Augmentation Using Keras\n\n<hr>\n\nHere I am using the ImageDataGenerator() function of Keras for Image augmentation. Parameters to use:\n\n* **rotation_range:**   randomly rotate images in the range (degrees, 0 to 180)\n* **zoom_range:**  Randomly zoom image \n* **width_shift_range:**  randomly shift images horizontally (fraction of total width)\n* **height_shift_range:**   randomly shift images vertically (fraction of total height)\n* **horizontal_flip:**   randomly flip images (Can't be used in this case as it changes the digit)\n* **vertical_flip:**  randomly flip images (Can't be used in this case as it changes the digit)\n\nRead more about [ImageDataGenerator()](https://keras.io/preprocessing/image/)\n\nAfter the creation and configuration of the ImageDataGenerator, you must fit it on the data, which calculates any statistics required to perform the transformation on the data. This can be done by calling the fit() function on datagen."},{"metadata":{"trusted":true},"cell_type":"code","source":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\ndatagen.fit(train_x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ImageDataGenerator()**\n\n* **featurewise_center**：布尔值，使输入数据集去中心化（均值为0）, 按feature执行。\n\n* **samplewise_center**：布尔值，使输入数据的每个样本均值为0。\n\n* **featurewise_std_normalization**：布尔值，将输入除以数据集的标准差以完成标准化, 按feature执行。\n\n* **samplewise_std_normalization**：布尔值，将输入的每个样本除以其自身的标准差。\n\n* **zca_whitening**：布尔值，对输入数据施加ZCA白化。\n\n* **rotation_range**：整数，数据提升时图片随机转动的角度。随机选择图片的角度，是一个0~180的度数，取值为0~180。\n\n* **width_shift_range**：浮点数，图片宽度的某个比例，数据提升时图片随机水平偏移的幅度。\n\n* **height_shift_range**：浮点数，图片高度的某个比例，数据提升时图片随机竖直偏移的幅度。 \n\n* **height_shift_range**和**width_shift_range**是用来指定水平和竖直方向随机移动的程度，这是两个0~1之间的比例。\n\n* **shear_range**：浮点数，剪切强度（逆时针方向的剪切变换角度）。是用来进行剪切变换的程度。\n\n* **zoom_range**：浮点数或形如[lower,upper]的列表，随机缩放的幅度，若为浮点数，则相当于[lower,upper] = [1 - zoom_range, 1+zoom_range]。用来进行随机的放大。\n\n* **channel_shift_range**：浮点数，随机通道偏移的幅度。\n\n* **fill_mode**：‘constant’，‘nearest’，‘reflect’或‘wrap’之一，当进行变换时超出边界的点将根据本参数给定的方法进行处理\n\n* **cval**：浮点数或整数，当fill_mode=constant时，指定要向超出边界的点填充的值。\n\n* **horizontal_flip**：布尔值，进行随机水平翻转。随机的对图片进行水平翻转，这个参数适用于水平翻转不影响图片语义的时候。\n\n* **vertical_flip**：布尔值，进行随机竖直翻转。\n\n* **rescale**: 值将在执行其他处理前乘到整个图像上，我们的图像在RGB通道都是0~255的整数，\n这样的操作可能使图像的值过高或过低，所以我们将这个值定为0~1之间的数。\n\n* **preprocessing_function**: 将被应用于每个输入的函数。该函数将在任何其他修改之前运行。该函数接受一个参数，为一张图片（秩为3的numpy array），并且输出一个具有相同shape的numpy array\n\n* **data_format**：字符串，“channel_first”或“channel_last”之一，代表图像的通道维的位置。该参数是Keras 1.x中的\n\n* **image_dim_ordering**，“channel_last”对应原本的“tf”，“channel_first”对应原本的“th”。以128x128的RGB图像为例，\n “channel_first”应将数据组织为（3,128,128），而“channel_last”应将数据组织为（128,128,3）。该参数的默认值是\n ~/.keras/keras.json中设置的值，若从未设置过，则为“channel_last”。"},{"metadata":{},"cell_type":"markdown","source":"### Learning Rate\n\n<hr>\n\nReduceLROnPlateau() is a callback function provided by Keras, which is used to reduce the learning rate if when a metric has stopped improving.\n\n<br>\n\n**Parameters:**\n\n* **monitor:**  takes the metric to observe (In this case val_accuracy)\n* **patience:** waits for that much epochs for the improvements, if not, then decrease the learning rate. (here `2`)\n* **factor:** factor by which the learning rate will be reduced. new_lr = lr * factor (here `0.5`)\n* **min_lr:** lower bound on the learning rate. (here `0.00001`)\n\n**Read More:**\n[Read more](https://keras.io/callbacks/#reducelronplateau)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"lrr = ReduceLROnPlateau(monitor='val_accuracy',patience=2,verbose=1,factor=0.5, min_lr=0.00001)\n# verbose：日志显示\n# verbose = 0 为不在标准输出流输出日志信息\n# verbose = 1 为输出进度条记录\n# verbose = 2 为每个epoch输出一行记录","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Further Traning\n[model.fit_generator()](https://keras.io/models/sequential/#fit_generator) is used to train the model on data generated batch-by-batch by image augmentation. The data generator is an iterator that generates and provides data as per request by fit_generator(). We can configure the batch size and get the batches by calling the flow() function.\n是用图像增强的方法对逐批生成的数据进行训练。\n"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"epochs = 20\nhistory_2 = model.fit_generator(datagen.flow(train_x,train_y, batch_size=batch_size),steps_per_epoch=int(train_x.shape[0]/batch_size)+1,epochs=epochs,validation_data=[val_x,val_y],callbacks=[lrr])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"model.save(\"model_img_augmentation.h5\")\nwith open('history_2.hs', 'wb') as history:\n    pickle.dump(history_2,history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"model = models.load_model(\"model_img_augmentation.h5\")\n# with open('history_2.hs', 'rb') as history:\n#     history_2 = pickle.load(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training Performance.\nNow we can see that after further training, the accuracy of training and validation set almost converges with high accuracy. It seems that the model has been significantly improved after image augmentation."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Diffining Figure\nf = plt.figure(figsize=(20,7))\nf.add_subplot(121)\n\n#Adding Subplot 1 (For Accuracy)\nplt.plot(history_1.epoch+list(np.asarray(history_2.epoch) + len(history_1.epoch)),history_1.history['accuracy']+history_2.history['accuracy'],label = \"accuracy\") # Accuracy curve for training set\nplt.plot(history_1.epoch+list(np.asarray(history_2.epoch) + len(history_1.epoch)),history_1.history['val_accuracy']+history_2.history['val_accuracy'],label = \"val_accuracy\") # Accuracy curve for validation set\n\nplt.title(\"Accuracy Curve\",fontsize=18)\nplt.xlabel(\"Epochs\",fontsize=15)\nplt.ylabel(\"Accuracy\",fontsize=15)\nplt.grid(alpha=0.3)\nplt.legend()\n\n\n#Adding Subplot 1 (For Loss)\nf.add_subplot(122)\n\nplt.plot(history_1.epoch+list(np.asarray(history_2.epoch) + len(history_1.epoch)),history_1.history['loss']+history_2.history['loss'],label=\"loss\") # Loss curve for training set\nplt.plot(history_1.epoch+list(np.asarray(history_2.epoch) + len(history_1.epoch)),history_1.history['val_loss']+history_2.history['val_loss'],label=\"val_loss\") # Loss curve for validation set\n\nplt.title(\"Loss Curve\",fontsize=18)\nplt.xlabel(\"Epochs\",fontsize=15)\nplt.ylabel(\"Loss\",fontsize=15)\nplt.grid(alpha=0.3)\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"val_p = np.argmax(model.predict(val_x),axis =1)\n\nerror = 0\nconfusion_matrix = np.zeros([10,10])\nfor i in range(val_x.shape[0]):\n    confusion_matrix[val_y[i],val_p[i]] += 1\n    if val_y[i]!=val_p[i]:\n        error +=1\n        \nconfusion_matrix,error,(error*100)/val_p.shape[0],100-(error*100)/val_p.shape[0],val_p.shape[0]\n\nprint(\"Confusion Matrix: \\n\\n\" ,confusion_matrix)\nprint(\"\\nErrors in validation set: \" ,error)\nprint(\"\\nError Persentage : \" ,(error*100)/val_p.shape[0])\nprint(\"\\nAccuracy : \" ,100-(error*100)/val_p.shape[0])\nprint(\"\\nValidation set Shape :\",val_p.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = plt.figure(figsize=(10,8.5))\nf.add_subplot(111)\n\nplt.imshow(np.log2(confusion_matrix+1),cmap=\"Reds\")\nplt.colorbar()\nplt.tick_params(size=5,color=\"white\")\nplt.xticks(np.arange(0,10),np.arange(0,10))\nplt.yticks(np.arange(0,10),np.arange(0,10))\n\nthreshold = confusion_matrix.max()/2 \n\nfor i in range(10):\n    for j in range(10):\n        plt.text(j,i,int(confusion_matrix[i,j]),horizontalalignment=\"center\",color=\"white\" if confusion_matrix[i, j] > threshold else \"black\")\n        \nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.savefig(\"Confusion_matrix2.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing Result\n\n<hr>\n\n### All Errors in the Validation set\nLet's see all the errors in the validation set. It seems that in most of the cases, the recognition of digits is difficult for even humans. So we can say that our model is performing well."},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = 4\ncols = 9\n\nf = plt.figure(figsize=(2*cols,2*rows))\nsub_plot = 1\nfor i in range(val_x.shape[0]):\n    if val_y[i]!=val_p[i]:\n        f.add_subplot(rows,cols,sub_plot) \n        sub_plot+=1\n        plt.imshow(val_x[i].reshape([28,28]),cmap=\"Blues\")\n        plt.axis(\"off\")\n        plt.title(\"T: \"+str(val_y[i])+\" P:\"+str(val_p[i]), y=-0.15,color=\"Red\")\nplt.savefig(\"error_plots.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict on Testset"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_y = np.argmax(model.predict(test_x),axis =1)\n# numpy.argmax(array, axis) 用于返回一个numpy数组中最大值的索引值","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.predict(test_x).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.argmax(model.predict(test_x),axis =1).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = 5\ncols = 10\n\nf = plt.figure(figsize=(2*cols,2*rows))\n\nfor i in range(rows*cols):\n    f.add_subplot(rows,cols,i+1)\n    plt.imshow(test_x[i].reshape([28,28]),cmap=\"Blues\")\n    plt.axis(\"off\")\n    plt.title(str(test_y[i]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets understand the intermediate layers of the model\n\n<hr>\n\n![hidden layers](https://i.stack.imgur.com/axn7z.jpg)\n\nTo visualize the output of each layer, we need to create a model to take input tensor and gives the list of output tensor, each representing the corresponding intermediate layers. To that, we need to create a multi-output model in which the input will be the image, and the output will be the list of intermediate layers.\n<br>\n为了可视化每一层的输出，我们需要创建一个模型来获取输入张量，并给出输出张量列表，每个张量表示相应的中间层。为此，我们需要创建一个多输出模型，其中输入为图像，输出为中间层列表。\n<br>\n\nI will use the Functional API of Keras to do so. When fed an image input, this model returns the values of the layer activations in the original model. \n<br>\n我将使用Keras的函数API来做到这一点。当输入图像时，该模型返回原始模型中的层激活值。\n<br>\n<br>\nI am taking all the intermediate layers except the Flatten and Dense layers. \n<br>\n我取了所有中间层，除了平坦和密集的层。\n<br>\n\n**model.layers** returns the list of layers of the model. Selecting all the layers except the last four layers. Then passing to **models.Model()** as a list of output layers and the input layer of the original model. It will return a new model having the input and output layers of the original model.\n<br>\n模型层返回模型的层列表。选择除最后四层以外的所有层。然后传给模型。模型（）作为原始模型的输出层和输入层的列表。它将返回一个具有原始模型的输入和输出层的新模型。\n\n\n**Check out these functions for more info:**\n* [Model()](https://keras.io/models/model/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extracts the outputs of all layers except Flatten and Dense layers\noutput_layers = [layer.output for layer in model.layers[:-4]]\n# Creates a model that will return these outputs, given the model input (This is multi output model)\nactivation_model = models.Model(inputs=model.input, outputs=output_layers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_layers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"activation_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Try to compare the layer 1 output \n<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# predicting the output of each layers\nactivations_2  = activation_model.predict(val_x[2].reshape([1,28,28,1]))\nactivations_6  = activation_model.predict(val_x[7].reshape([1,28,28,1]))\nfirst_activation_layer  = activations_2[0]\nfirst_activation_layer.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first_activation_layer6  = activations_6[0]\nfirst_activation_layer6.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**activation_2** and **activation_6** are the output of two different images. **first_activation_layer** represents the output of the first layer of the original model. The shape is (1,28,28,32) where 32 represent the number of **channels**. Let's compare the four different channels of the first layer of two different images."},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = 4\ncols = 2\n\nf = plt.figure(figsize=(2*cols,2*rows))\n\nfor i in range(4):\n    f.add_subplot(rows,cols,2*i+1)\n    plt.imshow(activations_2[0][0,:,:,i].reshape([28,28]),cmap=\"Blues\")\n    plt.axis(\"off\") \n\n    f.add_subplot(rows,cols,2*i+2)\n    plt.imshow(activations_6[0][0,:,:,i].reshape([28,28]),cmap=\"Blues\")\n    plt.savefig(\"layer_output_comparision\"+str(i)+\".png\")\n    plt.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that each channel is trying to depict something in both images, a kind of similar features. This can be concluded that each channel has been trained to find some specific features of the input images.\n\n## Visualizing All Intermediate Activation Layer可视化所有中间激活层"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_layer(layer,i,layer_name = None):\n    rows = layer.shape[-1]/16\n    cols = 16\n\n    f = plt.figure(figsize=(1*cols,1*rows))\n    # plt.imshow(first_activation_layer[0,:,:,:].reshape([14*4,14*16]),cmap=\"Blues\")\n    for i in range(layer.shape[-1]):\n        f.add_subplot(rows,cols,i+1)\n        plt.imshow(layer[0,:,:,i].reshape([layer.shape[1],layer.shape[2]]),cmap=\"Blues\")\n        plt.axis(\"off\")\n    f.suptitle(layer_name,fontsize=14)\n    plt.savefig(\"intermidiate_layers\"+str(i)+\".png\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualising each layers\nfor i,layer in enumerate(activation_model.predict(val_x[6].reshape([1,28,28,1]))):\n    plot_layer(layer,i,output_layers[i].name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, try to understand what's happening inside the hidden layers:\n* The first few layers retaining the shape of image but trying to get the very low-level features such as different types edges.\n* As we go into the deeper layers, the activations are more abstract and less visually interpretable. These layers are trying to encode high-level features such as corners, angles, selective borders, etc.\n* In the last few layers, We can't visually interpret anything; this is because layers are now encoding even more complex features, or we can say more information about the classes.\n\nI hope you understand the basics of CNN after the visualization of intermediate layers even more."},{"metadata":{},"cell_type":"markdown","source":"# Creating submisson"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission = pd.DataFrame([df_test.index+1,test_y],[\"ImageId\",\"Label\"]).transpose()\ndf_submission.to_csv(\"submission.csv\",index=False)\n# numpy.transpose()是对矩阵按照所需的要求的转置","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame([df_test.index+1,test_y],[\"ImageId\",\"Label\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame([df_test.index+1,test_y],[\"ImageId\",\"Label\"]).transpose()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# About Me\n<hr>\nI am Tarun Kumar from India.<br>\nSoftware Developer at Toppr.<br>\nHobbyist Artist. <br>\nPE Undergrad from IIT ISM Dhanbad. <br>\nDeep Learning, Reinforcement Learning, and Data Science. \n<br>\n<br>\n\n### Follow me:\n\n* <a href=\"https://bit.ly/tarungithub\"><img src=\"data:image/svg+xml;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCAyNCAyNCIgaGVpZ2h0PSI1MTIiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjUxMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMTIgLjVjLTYuNjMgMC0xMiA1LjI4LTEyIDExLjc5MiAwIDUuMjExIDMuNDM4IDkuNjMgOC4yMDUgMTEuMTg4LjYuMTExLjgyLS4yNTQuODItLjU2NyAwLS4yOC0uMDEtMS4wMjItLjAxNS0yLjAwNS0zLjMzOC43MTEtNC4wNDItMS41ODItNC4wNDItMS41ODItLjU0Ni0xLjM2MS0xLjMzNS0xLjcyNS0xLjMzNS0xLjcyNS0xLjA4Ny0uNzMxLjA4NC0uNzE2LjA4NC0uNzE2IDEuMjA1LjA4MiAxLjgzOCAxLjIxNSAxLjgzOCAxLjIxNSAxLjA3IDEuODAzIDIuODA5IDEuMjgyIDMuNDk1Ljk4MS4xMDgtLjc2My40MTctMS4yODIuNzYtMS41NzctMi42NjUtLjI5NS01LjQ2Ni0xLjMwOS01LjQ2Ni01LjgyNyAwLTEuMjg3LjQ2NS0yLjMzOSAxLjIzNS0zLjE2NC0uMTM1LS4yOTgtLjU0LTEuNDk3LjEwNS0zLjEyMSAwIDAgMS4wMDUtLjMxNiAzLjMgMS4yMDkuOTYtLjI2MiAxLjk4LS4zOTIgMy0uMzk4IDEuMDIuMDA2IDIuMDQuMTM2IDMgLjM5OCAyLjI4LTEuNTI1IDMuMjg1LTEuMjA5IDMuMjg1LTEuMjA5LjY0NSAxLjYyNC4yNCAyLjgyMy4xMiAzLjEyMS43NjUuODI1IDEuMjMgMS44NzcgMS4yMyAzLjE2NCAwIDQuNTMtMi44MDUgNS41MjctNS40NzUgNS44MTcuNDIuMzU0LjgxIDEuMDc3LjgxIDIuMTgyIDAgMS41NzgtLjAxNSAyLjg0Ni0uMDE1IDMuMjI5IDAgLjMwOS4yMS42NzguODI1LjU2IDQuODAxLTEuNTQ4IDguMjM2LTUuOTcgOC4yMzYtMTEuMTczIDAtNi41MTItNS4zNzMtMTEuNzkyLTEyLTExLjc5MnoiIGZpbGw9IiMyMTIxMjEiLz48L3N2Zz4=\" width=\"22\" align=\"left\" style=\"margin-right:10px\"/> GitHub</a>\n* <a href=\"https://bit.ly/tarnkr-youtube\"><img src=\"data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iaXNvLTg4NTktMSI/Pg0KPCEtLSBHZW5lcmF0b3I6IEFkb2JlIElsbHVzdHJhdG9yIDE5LjAuMCwgU1ZHIEV4cG9ydCBQbHVnLUluIC4gU1ZHIFZlcnNpb246IDYuMDAgQnVpbGQgMCkgIC0tPg0KPHN2ZyB2ZXJzaW9uPSIxLjEiIGlkPSJMYXllcl8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCINCgkgdmlld0JveD0iMCAwIDQ2MS4wMDEgNDYxLjAwMSIgc3R5bGU9ImVuYWJsZS1iYWNrZ3JvdW5kOm5ldyAwIDAgNDYxLjAwMSA0NjEuMDAxOyIgeG1sOnNwYWNlPSJwcmVzZXJ2ZSI+DQo8cGF0aCBzdHlsZT0iZmlsbDojRjYxQzBEOyIgZD0iTTM2NS4yNTcsNjcuMzkzSDk1Ljc0NEM0Mi44NjYsNjcuMzkzLDAsMTEwLjI1OSwwLDE2My4xMzd2MTM0LjcyOA0KCWMwLDUyLjg3OCw0Mi44NjYsOTUuNzQ0LDk1Ljc0NCw5NS43NDRoMjY5LjUxM2M1Mi44NzgsMCw5NS43NDQtNDIuODY2LDk1Ljc0NC05NS43NDRWMTYzLjEzNw0KCUM0NjEuMDAxLDExMC4yNTksNDE4LjEzNSw2Ny4zOTMsMzY1LjI1Nyw2Ny4zOTN6IE0zMDAuNTA2LDIzNy4wNTZsLTEyNi4wNiw2MC4xMjNjLTMuMzU5LDEuNjAyLTcuMjM5LTAuODQ3LTcuMjM5LTQuNTY4VjE2OC42MDcNCgljMC0zLjc3NCwzLjk4Mi02LjIyLDcuMzQ4LTQuNTE0bDEyNi4wNiw2My44ODFDMzA0LjM2MywyMjkuODczLDMwNC4yOTgsMjM1LjI0OCwzMDAuNTA2LDIzNy4wNTZ6Ii8+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8L3N2Zz4NCg==\" width=\"22\" align=\"left\" style=\"margin-right:10px\"/>Youtube<a/>\n* <a href=\"https://medium.com/@codeeasy\"><img src=\"data:image/svg+xml;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCAyNCAyNCIgaGVpZ2h0PSI1MTIiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjUxMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMjIuMDg1IDQuNzMzIDEuOTE1LTEuODMydi0uNDAxaC02LjYzNGwtNC43MjggMTEuNzY4LTUuMzc5LTExLjc2OGgtNi45NTZ2LjQwMWwyLjIzNyAyLjY5M2MuMjE4LjE5OS4zMzIuNDkuMzAzLjc4M3YxMC41ODNjLjA2OS4zODEtLjA1NS43NzMtLjMyMyAxLjA1bC0yLjUyIDMuMDU0di4zOTZoNy4xNDV2LS40MDFsLTIuNTItMy4wNDljLS4yNzMtLjI3OC0uNDAyLS42NjMtLjM0Ny0xLjA1di05LjE1NGw2LjI3MiAxMy42NTloLjcyOWw1LjM5My0xMy42NTl2MTAuODgxYzAgLjI4NyAwIC4zNDYtLjE4OC41MzRsLTEuOTQgMS44Nzd2LjQwMmg5LjQxMnYtLjQwMWwtMS44Ny0xLjgzMWMtLjE2NC0uMTI0LS4yNDktLjMzMi0uMjE0LS41MzR2LTEzLjQ2N2MtLjAzNS0uMjAzLjA0OS0uNDExLjIxMy0uNTM0eiIgZmlsbD0iIzIxMjEyMSIvPjwvc3ZnPg==\"  width=\"22\" align=\"left\" style=\"margin-right:10px\"/> Medium</a>\n* <a href=\"https://www.linkedin.com/in/tarun-kumar-iit-ism/\"><img src=\"data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iaXNvLTg4NTktMSI/Pg0KPCEtLSBHZW5lcmF0b3I6IEFkb2JlIElsbHVzdHJhdG9yIDE5LjAuMCwgU1ZHIEV4cG9ydCBQbHVnLUluIC4gU1ZHIFZlcnNpb246IDYuMDAgQnVpbGQgMCkgIC0tPg0KPHN2ZyB2ZXJzaW9uPSIxLjEiIGlkPSJMYXllcl8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCINCgkgdmlld0JveD0iMCAwIDM4MiAzODIiIHN0eWxlPSJlbmFibGUtYmFja2dyb3VuZDpuZXcgMCAwIDM4MiAzODI7IiB4bWw6c3BhY2U9InByZXNlcnZlIj4NCjxwYXRoIHN0eWxlPSJmaWxsOiMwMDc3Qjc7IiBkPSJNMzQ3LjQ0NSwwSDM0LjU1NUMxNS40NzEsMCwwLDE1LjQ3MSwwLDM0LjU1NXYzMTIuODg5QzAsMzY2LjUyOSwxNS40NzEsMzgyLDM0LjU1NSwzODJoMzEyLjg4OQ0KCUMzNjYuNTI5LDM4MiwzODIsMzY2LjUyOSwzODIsMzQ3LjQ0NFYzNC41NTVDMzgyLDE1LjQ3MSwzNjYuNTI5LDAsMzQ3LjQ0NSwweiBNMTE4LjIwNywzMjkuODQ0YzAsNS41NTQtNC41MDIsMTAuMDU2LTEwLjA1NiwxMC4wNTYNCglINjUuMzQ1Yy01LjU1NCwwLTEwLjA1Ni00LjUwMi0xMC4wNTYtMTAuMDU2VjE1MC40MDNjMC01LjU1NCw0LjUwMi0xMC4wNTYsMTAuMDU2LTEwLjA1Nmg0Mi44MDYNCgljNS41NTQsMCwxMC4wNTYsNC41MDIsMTAuMDU2LDEwLjA1NlYzMjkuODQ0eiBNODYuNzQ4LDEyMy40MzJjLTIyLjQ1OSwwLTQwLjY2Ni0xOC4yMDctNDAuNjY2LTQwLjY2NlM2NC4yODksNDIuMSw4Ni43NDgsNDIuMQ0KCXM0MC42NjYsMTguMjA3LDQwLjY2Niw0MC42NjZTMTA5LjIwOCwxMjMuNDMyLDg2Ljc0OCwxMjMuNDMyeiBNMzQxLjkxLDMzMC42NTRjMCw1LjEwNi00LjE0LDkuMjQ2LTkuMjQ2LDkuMjQ2SDI4Ni43Mw0KCWMtNS4xMDYsMC05LjI0Ni00LjE0LTkuMjQ2LTkuMjQ2di04NC4xNjhjMC0xMi41NTYsMy42ODMtNTUuMDIxLTMyLjgxMy01NS4wMjFjLTI4LjMwOSwwLTM0LjA1MSwyOS4wNjYtMzUuMjA0LDQyLjExdjk3LjA3OQ0KCWMwLDUuMTA2LTQuMTM5LDkuMjQ2LTkuMjQ2LDkuMjQ2aC00NC40MjZjLTUuMTA2LDAtOS4yNDYtNC4xNC05LjI0Ni05LjI0NlYxNDkuNTkzYzAtNS4xMDYsNC4xNC05LjI0Niw5LjI0Ni05LjI0Nmg0NC40MjYNCgljNS4xMDYsMCw5LjI0Niw0LjE0LDkuMjQ2LDkuMjQ2djE1LjY1NWMxMC40OTctMTUuNzUzLDI2LjA5Ny0yNy45MTIsNTkuMzEyLTI3LjkxMmM3My41NTIsMCw3My4xMzEsNjguNzE2LDczLjEzMSwxMDYuNDcyDQoJTDM0MS45MSwzMzAuNjU0TDM0MS45MSwzMzAuNjU0eiIvPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPC9zdmc+DQo=\" width=\"22\" align=\"left\" style=\"margin-right:10px\"/>Linkedin</a>\n\n\n<hr>\n\n# Feedback\n* **Your feedback is much appreciated**\n* **Please UPVOTE if you LIKE this notebook**\n* **Comment if you have any doubts or you found any errors in the notebook**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}